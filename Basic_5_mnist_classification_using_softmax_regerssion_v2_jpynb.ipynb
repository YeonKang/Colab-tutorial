{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_5_mnist_classification_using_softmax_regerssion_v2.jpynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLKAvXHb+dtGEuTY9u28aW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNS3mvPE7OkT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c1a9146-e8c5-4d41-8996-d62c7a5101a0"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#download MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "#transform images into float32 data type\n",
        "x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n",
        "#flattening 28*28 image into 784 dimensions\n",
        "x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\n",
        "#normalize [0, 255] into [0, 1]\n",
        "x_train, x_test = x_train / 255., x_test / 255.\n",
        "#adapt one-hot encoding to lable data\n",
        "y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)\n",
        "\n",
        "#shuffle data using tf.data API\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.repeat().shuffle(60000).batch(100)\n",
        "train_data_iter = iter(train_data)\n",
        "\n",
        "#set tf.Variable for Softmax Regression model\n",
        "W = tf.Variable(tf.zeros(shape=[784, 10]))\n",
        "b = tf.Variable(tf.zeros(shape=[10]))\n",
        "\n",
        "#define Softmax Regression model\n",
        "@tf.function\n",
        "def softmax_regression(x):\n",
        "  logits = tf.matmul(x, W) + b\n",
        "  return tf.nn.softmax(logits)\n",
        "\n",
        "#define cross-antropy loss function\n",
        "@tf.function\n",
        "def cross_entropy_loss(y_pred, y):\n",
        "  return tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(y_pred), axis=[1]))\n",
        "  #with tf.nn.softmax_cross_entropy_with_logits API\n",
        "  #return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logtis, labels=y))\n",
        "\n",
        "#define accuracy function\n",
        "@tf.function\n",
        "def compute_accuracy(y_pred, y):\n",
        "  correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "#set gradient descent optimizer for oprimize\n",
        "optimizer = tf.optimizers.SGD(0.5)\n",
        "\n",
        "#define optimization function\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = softmax_regression(x)\n",
        "    loss = cross_entropy_loss(y_pred, y)\n",
        "  gradients = tape.gradient(loss, [W, b])\n",
        "  optimizer.apply_gradients(zip(gradients, [W, b]))\n",
        "\n",
        "#repeat 1000 times to optimize\n",
        "for i in range(1000):\n",
        "  batch_xs, batch_ys = next(train_data_iter)\n",
        "  train_step(batch_xs, batch_ys)\n",
        "\n",
        "print(\"Accuracy: %f\" % compute_accuracy(softmax_regression(x_test), y_test))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.917200\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}