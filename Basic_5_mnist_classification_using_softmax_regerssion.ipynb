{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic - 5. mnist_classification_using_softmax_regerssion.ipynb",
      "provenance": [],
      "mount_file_id": "1naI6E4yGf6qOCndgfw_5c2B05xFgvZ_p",
      "authorship_tag": "ABX9TyOWSIZdH80js5NTf2i0DP1s"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9yM354pKOrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import tensorflow version 1.x\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "#if you want to use tensorflow version 2, use below\n",
        "#import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "\n",
        "#download MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True) #\"one_hot=True\" means I will use one_hot encoding (encode Catagorical Value into Binary Value)\n",
        "\n",
        "#define placeholder\n",
        "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "#define variables and softmax regression model\n",
        "W = tf.Variable(tf.zeros(shape=[784, 10]))\n",
        "b = tf.Variable(tf.zeros(shape=[10]))\n",
        "logits = tf.matmul(x,W) + b\n",
        "y_pred = tf.nn.softmax(logits)\n",
        "\n",
        "#define cross-entropy loss function and optimizer\n",
        "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)) -> use 'tf.nn.softmax cross entropy with ligits' API\n",
        "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_pred), reduction_indices=[1]))\n",
        "train_step = tf.train.GradientDexcentOptimizer(0.5).minimize(loss)\n",
        "\n",
        "#open session and initialize variables\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#run 1000 times\n",
        "for i in range (1000):\n",
        "  batch_xs, batch_ys = mnist.train.next_batch(100) #use mini-batch gradient descent in every 100\n",
        "  sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys})\n",
        "\n",
        "#after training, output the accuracy of the trained model\n",
        "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print(\"Accuracy: %f\" %sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels}))\n",
        "\n",
        "#close session\n",
        "sess.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST1WJ1pmQKzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}