{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab12_6_seq_to_seq_with_attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP/4JgkFK5jnKsSEebTuxH4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YeonKang/Tensorflow-with-Colab/blob/master/Lab12_6_seq_to_seq_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk8zv-2OIkPz"
      },
      "source": [
        "**simple neural machine translation training**\n",
        "\n",
        "*   sequence to sequence\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0c6ein46r34"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "### matplotlib 한글 폰트 설정 #############################\n",
        "from matplotlib import font_manager, rc\n",
        "# for window\n",
        "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
        "rc('font', family=font_name)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kndh42-DI5YD"
      },
      "source": [
        "sources = [['I', 'feel', 'hungry'],\n",
        "     ['tensorflow', 'is', 'very', 'difficult'],\n",
        "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
        "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
        "targets = [['나는', '배가', '고프다'],\n",
        "           ['텐서플로우는', '매우', '어렵다'],\n",
        "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
        "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s92UEO1tI8pU"
      },
      "source": [
        "# vocabulary for sources\n",
        "s_vocab = list(set(sum(sources, [])))\n",
        "s_vocab.sort()\n",
        "s_vocab = ['<pad>'] + s_vocab\n",
        "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
        "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
        "\n",
        "pprint(source2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En8KDJcHI_XJ"
      },
      "source": [
        "# vocabulary for targets\n",
        "t_vocab = list(set(sum(targets, [])))\n",
        "t_vocab.sort()\n",
        "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n",
        "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
        "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
        "\n",
        "pprint(target2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHNgU77NJCdY"
      },
      "source": [
        "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
        "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
        "    \n",
        "    if mode == 'source':\n",
        "        # preprocessing for source (encoder)\n",
        "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
        "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
        "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
        "        return s_len, s_input\n",
        "    \n",
        "    elif mode == 'target':\n",
        "        # preprocessing for target (decoder)\n",
        "        # input\n",
        "        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
        "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
        "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
        "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
        "        \n",
        "        # output\n",
        "        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
        "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
        "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
        "        \n",
        "        return t_len, t_input, t_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw-cUyNsJFbm"
      },
      "source": [
        "# preprocessing for source\n",
        "s_max_len = 10\n",
        "s_len, s_input = preprocess(sequences = sources,\n",
        "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n",
        "print(s_len, s_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL1ukTulJHpS"
      },
      "source": [
        "# preprocessing for target\n",
        "t_max_len = 12\n",
        "t_len, t_input, t_output = preprocess(sequences = targets,\n",
        "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
        "print(t_len, t_input, t_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dklnIksCKLvV"
      },
      "source": [
        "**hyper-param**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2ZRvuQ0KORs"
      },
      "source": [
        "# hyper-parameters\n",
        "epochs = 100\n",
        "batch_size = 4\n",
        "learning_rate = .005\n",
        "total_step = epochs / batch_size\n",
        "buffer_size = 100\n",
        "n_batch = buffer_size//batch_size\n",
        "embedding_dim = 32\n",
        "units = 128\n",
        "\n",
        "# input\n",
        "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
        "data = data.shuffle(buffer_size = buffer_size)\n",
        "data = data.batch(batch_size = batch_size)\n",
        "# s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7Iyi0MVKQLZ"
      },
      "source": [
        "def gru(units):\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}