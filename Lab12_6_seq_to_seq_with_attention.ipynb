{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab12_6_seq_to_seq_with_attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlORnw/WtZ1pYxQ2aucnSA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YeonKang/Tensorflow-with-Colab/blob/master/Lab12_6_seq_to_seq_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk8zv-2OIkPz"
      },
      "source": [
        "**simple neural machine translation training**\n",
        "\n",
        "*   sequence to sequence\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0c6ein46r34",
        "outputId": "cbf060ba-5198-4d9d-ffd8-c0b3cefa4898"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "### matplotlib 한글 폰트 설정 #############################\n",
        "from matplotlib import font_manager, rc\n",
        "# for window\n",
        "#font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/Arial.ttf\").get_name()\n",
        "#rc('font', family=font_name)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kndh42-DI5YD"
      },
      "source": [
        "sources = [['I', 'feel', 'hungry'],\n",
        "     ['tensorflow', 'is', 'very', 'difficult'],\n",
        "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
        "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
        "targets = [['나는', '배가', '고프다'],\n",
        "           ['텐서플로우는', '매우', '어렵다'],\n",
        "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
        "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s92UEO1tI8pU",
        "outputId": "2b619d61-3273-4443-aa3d-64d0eb4542cb"
      },
      "source": [
        "# vocabulary for sources\n",
        "s_vocab = list(set(sum(sources, [])))\n",
        "s_vocab.sort()\n",
        "s_vocab = ['<pad>'] + s_vocab\n",
        "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
        "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
        "\n",
        "pprint(source2idx)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0,\n",
            " 'I': 1,\n",
            " 'a': 2,\n",
            " 'changing': 3,\n",
            " 'deep': 4,\n",
            " 'difficult': 5,\n",
            " 'fast': 6,\n",
            " 'feel': 7,\n",
            " 'for': 8,\n",
            " 'framework': 9,\n",
            " 'hungry': 10,\n",
            " 'is': 11,\n",
            " 'learning': 12,\n",
            " 'tensorflow': 13,\n",
            " 'very': 14}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En8KDJcHI_XJ",
        "outputId": "8703c00a-e9a2-469b-9973-f6ea876cbb49"
      },
      "source": [
        "# vocabulary for targets\n",
        "t_vocab = list(set(sum(targets, [])))\n",
        "t_vocab.sort()\n",
        "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n",
        "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
        "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
        "\n",
        "pprint(target2idx)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<bos>': 1,\n",
            " '<eos>': 2,\n",
            " '<pad>': 0,\n",
            " '고프다': 3,\n",
            " '나는': 4,\n",
            " '딥러닝을': 5,\n",
            " '매우': 6,\n",
            " '배가': 7,\n",
            " '변화한다': 8,\n",
            " '빠르게': 9,\n",
            " '어렵다': 10,\n",
            " '위한': 11,\n",
            " '텐서플로우는': 12,\n",
            " '프레임워크이다': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHNgU77NJCdY"
      },
      "source": [
        "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
        "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
        "    \n",
        "    if mode == 'source':\n",
        "        # preprocessing for source (encoder)\n",
        "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
        "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
        "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
        "        return s_len, s_input\n",
        "    \n",
        "    elif mode == 'target':\n",
        "        # preprocessing for target (decoder)\n",
        "        # input\n",
        "        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
        "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
        "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
        "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
        "        \n",
        "        # output\n",
        "        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
        "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
        "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
        "        \n",
        "        return t_len, t_input, t_output"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kw-cUyNsJFbm",
        "outputId": "f2ca23dc-da30-416c-f49a-63f84c7d5c55"
      },
      "source": [
        "# preprocessing for source\n",
        "s_max_len = 10\n",
        "s_len, s_input = preprocess(sequences = sources,\n",
        "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n",
        "print(s_len, s_input)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 4, 7, 5] [[ 1  7 10  0  0  0  0  0  0  0]\n",
            " [13 11 14  5  0  0  0  0  0  0]\n",
            " [13 11  2  9  8  4 12  0  0  0]\n",
            " [13 11 14  6  3  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL1ukTulJHpS",
        "outputId": "24abf6de-73d6-4e50-d71a-bc91740eed25"
      },
      "source": [
        "# preprocessing for target\n",
        "t_max_len = 12\n",
        "t_len, t_input, t_output = preprocess(sequences = targets,\n",
        "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
        "print(t_len, t_input, t_output)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 5, 6, 6] [[ 1  4  7  3  2  0  0  0  0  0  0  0]\n",
            " [ 1 12  6 10  2  0  0  0  0  0  0  0]\n",
            " [ 1 12  5 11 13  2  0  0  0  0  0  0]\n",
            " [ 1 12  6  9  8  2  0  0  0  0  0  0]] [[ 4  7  3  2  0  0  0  0  0  0  0  0]\n",
            " [12  6 10  2  0  0  0  0  0  0  0  0]\n",
            " [12  5 11 13  2  0  0  0  0  0  0  0]\n",
            " [12  6  9  8  2  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dklnIksCKLvV"
      },
      "source": [
        "**hyper-param**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2ZRvuQ0KORs"
      },
      "source": [
        "# hyper-parameters\n",
        "epochs = 100\n",
        "batch_size = 4\n",
        "learning_rate = .005\n",
        "total_step = epochs / batch_size\n",
        "buffer_size = 100\n",
        "n_batch = buffer_size//batch_size\n",
        "embedding_dim = 32\n",
        "units = 128\n",
        "\n",
        "# input\n",
        "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
        "data = data.shuffle(buffer_size = buffer_size)\n",
        "data = data.batch(batch_size = batch_size)\n",
        "# s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7Iyi0MVKQLZ"
      },
      "source": [
        "def gru(units):\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo3N9aAQP6fu"
      },
      "source": [
        "**hyper-param**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56srimo0P-Ha"
      },
      "source": [
        "epochs = 100\n",
        "batch_size = 4\n",
        "learning_rate = .005\n",
        "total_step = epochs / batch_size\n",
        "buffer_size = 100\n",
        "n_batch = buffer_size//batch_size\n",
        "embedding_dim = 32\n",
        "units = 128\n",
        "\n",
        "# input\n",
        "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
        "data = data.shuffle(buffer_size = buffer_size)\n",
        "data = data.batch(batch_size = batch_size)\n",
        "# s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2AYB2_VQAL5"
      },
      "source": [
        "def gru(units):\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l-2Lk5rQB2i"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.enc_units)\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "         # print(\"state : {}\".format(state.shape))\n",
        "         # print(\"output: {}\".format(state.shape))\n",
        "              \n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEqQdyLFQDwW"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.dec_units)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        # * `score = FC(tanh(FC(EO) + FC(H)))`\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "                \n",
        "        #* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        # * `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        # * `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        # * `merged vector = concat(embedding output, context vector)`\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC0J-k7QQH4W"
      },
      "source": [
        "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
        "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = 1 - np.equal(real, 0)\n",
        "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "    \n",
        "#     print(\"real: {}\".format(real))\n",
        "#     print(\"pred: {}\".format(pred))\n",
        "#     print(\"mask: {}\".format(mask))\n",
        "#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "# creating optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# creating check point (Object-based saving)\n",
        "checkpoint_dir = './data_out/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                encoder=encoder,\n",
        "                                decoder=decoder)\n",
        "\n",
        "# create writer for tensorboard\n",
        "summary_writer = tf.summary.create_file_writer(logdir=checkpoint_dir)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XomFQIhsQKQx",
        "outputId": "a58119e6-3311-4d32-ebad-0d4638d7ed5b"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n",
        "        loss = 0\n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(s_input, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([target2idx['<bos>']] * batch_size, 1)\n",
        "            \n",
        "            #Teacher Forcing: feeding the target as the next input\n",
        "            for t in range(1, t_input.shape[1]):\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(t_input[:, t], predictions)\n",
        "            \n",
        "                dec_input = tf.expand_dims(t_input[:, t], 1) #using teacher forcing\n",
        "                \n",
        "        batch_loss = (loss / int(t_input.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradient = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradient, variables))\n",
        "        \n",
        "    if epoch % 10 == 0:\n",
        "        #save model every 10 epoch\n",
        "        print('Epoch {} Loss {:.4f} Batch Loss {:.4f}'.format(epoch,\n",
        "                                            total_loss / n_batch,\n",
        "                                            batch_loss.numpy()))\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss 0.0396 Batch Loss 0.9890\n",
            "Epoch 10 Loss 0.0376 Batch Loss 0.9409\n",
            "Epoch 20 Loss 0.0343 Batch Loss 0.8585\n",
            "Epoch 30 Loss 0.0328 Batch Loss 0.8205\n",
            "Epoch 40 Loss 0.0300 Batch Loss 0.7498\n",
            "Epoch 50 Loss 0.0233 Batch Loss 0.5821\n",
            "Epoch 60 Loss 0.0180 Batch Loss 0.4495\n",
            "Epoch 70 Loss 0.0133 Batch Loss 0.3325\n",
            "Epoch 80 Loss 0.0086 Batch Loss 0.2139\n",
            "Epoch 90 Loss 0.0049 Batch Loss 0.1229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvGj6itEQM2D"
      },
      "source": [
        "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    \n",
        "#     sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang['<bos>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "        \n",
        "        # storing the attention weigths to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += idx2target[predicted_id] + ' '\n",
        "\n",
        "        if idx2target.get(predicted_id) == '<eos>':\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "# result, sentence, attention_plot = evaluate(sentence, encoder, decoder, source2idx, target2idx,\n",
        "#                                             s_max_len, t_max_len)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9hTId-6QQFX"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    \n",
        "    fontdict = {'fontsize': 14}\n",
        "    \n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJCHVCQQQRvz"
      },
      "source": [
        "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
        "        \n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    \n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDJuXYPxQTRk",
        "outputId": "d5939b2f-8aa4-47c6-f945-72ca8a4c866f"
      },
      "source": [
        "#restore checkpoint\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6fe832f150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pdOxVrv1QWN7",
        "outputId": "d72bbe3e-da38-4f8b-bf5b-69009b6d0e10"
      },
      "source": [
        "sentence = 'I feel hungry'\n",
        "# sentence = 'tensorflow is a framework for deep learning'\n",
        "\n",
        "translate(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: I feel hungry\n",
            "Predicted translation: 나는 배가 고프다 <eos> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45208 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45716 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48176 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44032 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54532 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45208 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45716 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48176 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44032 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54532 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}